"{\"class_name\": \"Tokenizer\", \"config\": {\"num_words\": 100000, \"filters\": \"!\\\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\", \"lower\": true, \"split\": \" \", \"char_level\": false, \"oov_token\": \"<OOV>\", \"document_count\": 41782, \"word_counts\": \"{\\\"2021.0\\\": 13798, \\\"2022.0\\\": 2742, \\\"2020.0\\\": 17844, \\\"nan\\\": 7398}\", \"word_docs\": \"{\\\"2021.0\\\": 13798, \\\"2022.0\\\": 2742, \\\"2020.0\\\": 17844, \\\"nan\\\": 7398}\", \"index_docs\": \"{\\\"3\\\": 13798, \\\"5\\\": 2742, \\\"2\\\": 17844, \\\"4\\\": 7398}\", \"index_word\": \"{\\\"1\\\": \\\"<OOV>\\\", \\\"2\\\": \\\"2020.0\\\", \\\"3\\\": \\\"2021.0\\\", \\\"4\\\": \\\"nan\\\", \\\"5\\\": \\\"2022.0\\\"}\", \"word_index\": \"{\\\"<OOV>\\\": 1, \\\"2020.0\\\": 2, \\\"2021.0\\\": 3, \\\"nan\\\": 4, \\\"2022.0\\\": 5}\"}}"