"{\"class_name\": \"Tokenizer\", \"config\": {\"num_words\": 100000, \"filters\": \"!\\\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\", \"lower\": true, \"split\": \" \", \"char_level\": false, \"oov_token\": \"<OOV>\", \"document_count\": 200000, \"word_counts\": \"{\\\"no\\\": 194267, \\\"nodefinido\\\": 5555, \\\"si\\\": 178}\", \"word_docs\": \"{\\\"no\\\": 194267, \\\"nodefinido\\\": 5555, \\\"si\\\": 178}\", \"index_docs\": \"{\\\"2\\\": 194267, \\\"3\\\": 5555, \\\"4\\\": 178}\", \"index_word\": \"{\\\"1\\\": \\\"<OOV>\\\", \\\"2\\\": \\\"no\\\", \\\"3\\\": \\\"nodefinido\\\", \\\"4\\\": \\\"si\\\"}\", \"word_index\": \"{\\\"<OOV>\\\": 1, \\\"no\\\": 2, \\\"nodefinido\\\": 3, \\\"si\\\": 4}\"}}"